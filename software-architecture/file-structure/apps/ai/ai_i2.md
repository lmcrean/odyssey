# Apps/AI - Odyssey Creator Platform (Iteration 2)

> **Advanced AI Service** - Sophisticated chat, memory, content generation, and AI features for scale

## Architecture Overview

The AI app is **intentionally separated** from the main API to support advanced AI features and independent scaling as AI technology evolves rapidly through 2025-2035. This iteration adds all the sophisticated features that were excluded from MVP.

## Advanced Message Flow

```typescript
ðŸ“± Frontend (apps/web)
    â†“ POST /api/chat/send-message { message, userId, conversationId?, assessmentId? }
ðŸ”§ API (apps/api) 
    â†“ Saves user message to DB
    â†“ Retrieves conversation history & context
    â†“ HTTP POST to odyssey-ai-lmcreans-projects.vercel.app/api/chat/send-message
ðŸ¤– AI Service (apps/ai)
    â†“ Assessment context integration
    â†“ Conversation history processing
    â†“ Dynamic prompt building
    â†“ Context-aware Gemini processing
    â†‘ Returns AI response { content, metadata, contextUsed }
ðŸ”§ API (apps/api)
    â†‘ Saves AI message with metadata
    â†‘ Updates conversation context
    â†‘ Returns complete conversation to Frontend
ðŸ“± Frontend receives: { userMessage, aiMessage, conversationId, contextSummary }
```

## Deployment Structure

```typescript
// Separate Vercel deployment with enhanced capabilities
apps/ai/ â†’ odyssey-ai-lmcreans-projects.vercel.app

// Service communication with context awareness
apps/web/ â†’ apps/api/ â†’ apps/ai/
              â†“         â†‘
         Database   AI Processing
         (Stores)   (Context-Aware)
```

## Advanced Directory Structure

```typescript
apps/ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ apps/
â”‚   â”‚   â”œâ”€â”€ chat/                    # Core conversational AI functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ send-initial-message/    # New conversation starter
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Controller.ts        # Context-aware initial responses
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Route.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ send-follow-up-message/  # Ongoing conversation
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Controller.ts        # Context-aware follow-ups
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Route.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ get-conversation-context/ # Context retrieval for API
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Controller.ts
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Route.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ validate-conversation/    # Conversation state validation
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Controller.ts
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ Route.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ GeminiService.ts          # Enhanced Gemini API integration
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ConversationProcessor.ts  # Conversation state management
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ PromptBuilder.ts          # Dynamic prompt generation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ResponseProcessor.ts      # AI response processing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ContextManager.ts         # Conversation context handling
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ AssessmentHandler.ts      # Assessment integration logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FallbackManager.ts        # Advanced fallback responses
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatOrchestrator.ts       # Orchestrates complex chat flows
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ConversationManager.ts    # Manages conversation lifecycle
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ConversationValidator.ts  # Validate conversation state
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ContextInjector.ts        # Inject conversation context
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ResponseFormatter.ts      # Standardize response format
â”‚   â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚   â”‚       â”œâ”€â”€ conversation.ts           # Conversation state types
â”‚   â”‚   â”‚       â”œâ”€â”€ context.ts               # Context management types
â”‚   â”‚   â”‚       â”œâ”€â”€ assessment.ts            # Assessment integration types
â”‚   â”‚   â”‚       â”œâ”€â”€ prompts.ts               # Prompt building types
â”‚   â”‚   â”‚       â”œâ”€â”€ gemini.ts                # Gemini-specific types
â”‚   â”‚   â”‚       â””â”€â”€ responses.ts             # Response format types
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ memory/                  # ðŸš€ Advanced AI memory system
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ store-memory/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retrieve-context/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ update-personality/
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MemoryService.ts     # User-specific memory storage
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ContextService.ts    # Conversation context management
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ PersonalityService.ts # AI personality evolution
â”‚   â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚   â”‚       â”œâ”€â”€ UserMemory.ts
â”‚   â”‚   â”‚       â”œâ”€â”€ ConversationContext.ts
â”‚   â”‚   â”‚       â””â”€â”€ AIPersonality.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ content/                 # ðŸš€ AI content generation
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate-caption/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ suggest-hashtags/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ analyze-image/
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ContentGenerator.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ImageAnalyzer.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TrendAnalyzer.ts
â”‚   â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚   â”‚       â””â”€â”€ content-generation.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ agents/                  # ðŸš€ Future: AI agents
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ schedule-post/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auto-respond/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ analyze-engagement/
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SchedulingAgent.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EngagementAgent.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ModeratorAgent.ts
â”‚   â”‚   â”‚   â””â”€â”€ workflows/
â”‚   â”‚   â”‚       â”œâ”€â”€ PostingWorkflow.ts
â”‚   â”‚   â”‚       â””â”€â”€ ModerationWorkflow.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ multimodal/              # ðŸš€ Future: Advanced AI
â”‚   â”‚       â”œâ”€â”€ routes/
â”‚   â”‚       â”‚   â”œâ”€â”€ process-video/
â”‚   â”‚       â”‚   â”œâ”€â”€ generate-voice/
â”‚   â”‚       â”‚   â””â”€â”€ create-thumbnail/
â”‚   â”‚       â”œâ”€â”€ services/
â”‚   â”‚       â”‚   â”œâ”€â”€ VideoProcessor.ts
â”‚   â”‚       â”‚   â”œâ”€â”€ VoiceGenerator.ts
â”‚   â”‚       â”‚   â””â”€â”€ ImageGenerator.ts
â”‚   â”‚       â””â”€â”€ types/
â”‚   â”‚           â””â”€â”€ multimodal.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.config.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.config.ts            # Advanced system prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ safety.config.ts             # Safety settings
â”‚   â”‚   â”‚   â”œâ”€â”€ openai.config.ts             # Multiple AI providers
â”‚   â”‚   â”‚   â””â”€â”€ anthropic.config.ts          # Claude integration
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.ts                      # Internal API token validation
â”‚   â”‚   â”‚   â”œâ”€â”€ rateLimit.ts                 # Advanced rate limiting
â”‚   â”‚   â”‚   â”œâ”€â”€ errorHandler.ts              # Centralized error handling
â”‚   â”‚   â”‚   â””â”€â”€ cost-tracking.ts             # AI cost monitoring
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ AIProviderRouter.ts          # Route to best AI model
â”‚   â”‚   â”‚   â”œâ”€â”€ CostOptimizer.ts             # Optimize AI costs
â”‚   â”‚   â”‚   â””â”€â”€ ResponseCache.ts             # Cache AI responses
â”‚   â”‚   â”œâ”€â”€ utilities/
â”‚   â”‚   â”‚   â”œâ”€â”€ messageUtils.ts              # Message formatting utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ contextUtils.ts              # Context manipulation utilities
â”‚   â”‚   â”‚   â””â”€â”€ validationUtils.ts           # Input validation utilities
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â”œâ”€â”€ ai-provider.ts
â”‚   â”‚       â”œâ”€â”€ cost-tracking.ts
â”‚   â”‚       â”œâ”€â”€ api.ts                       # API request/response types
â”‚   â”‚       â”œâ”€â”€ internal.ts                  # Internal service types
â”‚   â”‚       â””â”€â”€ common.ts                    # Shared common types
â”‚   â”‚
â”‚   â””â”€â”€ server.ts
â”œâ”€â”€ vercel.json
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

## Complex Conversation Processing

### **Advanced Conversation Processor**

```typescript
// apps/ai/src/apps/chat/services/ConversationProcessor.ts

export class ConversationProcessor {
  
  // Handle initial message with assessment context
  async processInitialMessage(request: InitialMessageRequest): Promise<AIResponse> {
    // 1. Assessment context integration
    const assessmentContext = await this.assessmentHandler.getContext(request.assessmentId);
    
    // 2. Build enhanced prompt with assessment data
    const systemPrompt = this.promptBuilder.buildInitialPrompt(assessmentContext);
    
    // 3. Process with Gemini
    const geminiResponse = await this.geminiService.sendInitialMessage(
      systemPrompt,
      request.message,
      assessmentContext
    );
    
    // 4. Process and format response
    return this.responseProcessor.formatInitialResponse(geminiResponse, assessmentContext);
  }
  
  // Handle follow-up with conversation history
  async processFollowUpMessage(request: FollowUpMessageRequest): Promise<AIResponse> {
    // 1. Process conversation history
    const contextSummary = this.contextManager.buildContextSummary(request.conversationHistory);
    
    // 2. Convert to Gemini format
    const geminiHistory = this.contextManager.formatForGemini(request.conversationHistory);
    
    // 3. Build context-aware prompt
    const enhancedPrompt = this.promptBuilder.buildFollowUpPrompt(contextSummary);
    
    // 4. Process with conversation context
    const geminiResponse = await this.geminiService.sendFollowUpMessage(
      geminiHistory,
      request.message,
      enhancedPrompt
    );
    
    // 5. Process and format response
    return this.responseProcessor.formatFollowUpResponse(geminiResponse, contextSummary);
  }
}
```

### **Assessment Integration**

```typescript
// apps/ai/src/apps/chat/services/AssessmentHandler.ts

export class AssessmentHandler {
  
  // Extract assessment context for AI
  async getAssessmentContext(assessmentId: string): Promise<AssessmentContext> {
    // Complex logic from archive:
    // - Assessment pattern recognition
    // - Symptom context extraction  
    // - Personalization data
    // - Medical context awareness
    
    return {
      pattern: 'irregular_periods',
      symptoms: ['heavy_flow', 'pain'],
      personalizedGuidance: true,
      medicalDisclaimerRequired: true
    };
  }
  
  // Build assessment-aware prompts
  buildAssessmentPrompt(assessmentData: AssessmentContext): string {
    // From archive: complex prompt building with:
    // - User's specific assessment results
    // - Personalized system prompts
    // - Medical disclaimer integration
    // - Context-specific guidance
    
    return `You are Dottie, responding to a user with ${assessmentData.pattern}...`;
  }
}
```

### **Advanced Fallback Management**

```typescript
// apps/ai/src/apps/chat/services/FallbackManager.ts

export class FallbackManager {
  
  // Generate contextual mock responses (from archive)
  generateInitialFallback(message: string, assessmentId?: string): string {
    // Complex fallback logic based on:
    // - Message content analysis
    // - Assessment context
    // - Conversation type
    
    if (message.includes('irregular')) {
      return "Irregular periods can have many causes...";
    }
    // ... more complex logic
  }
  
  // Generate follow-up fallbacks
  generateFollowUpFallback(message: string, conversationHistory: Message[]): string {
    // Context-aware fallbacks based on:
    // - Previous conversation
    // - Current message intent
    // - Conversation flow
  }
}
```

## API Endpoints & Complex Flows

### **Advanced API Endpoints**

```typescript
// 1. Initial Message Processing
POST /api/chat/send-initial-message
{
  userId: string;
  message: string;
  conversationId: string;
  assessmentId?: string;        # Links to user assessment
  assessmentData?: object;      # Assessment context
}

// 2. Follow-up Message Processing  
POST /api/chat/send-follow-up-message
{
  userId: string;
  message: string;
  conversationId: string;
  conversationHistory: Message[];  # Last 10-20 messages for context
}

// 3. Conversation Context Retrieval
GET /api/chat/conversation-context/:conversationId
# Returns: conversation state, assessment links, context summary

// 4. Health/Status Check
GET /api/chat/health
# Returns: service status, Gemini API availability
```

## Enhanced API Integration Pattern

```typescript
// apps/api/src/apps/chat/services/AIService.ts

export class AIService {
  private aiBaseUrl = 'https://odyssey-ai-lmcreans-projects.vercel.app';
  
  async processMessage(userId: string, message: string, conversationHistory: Message[]) {
    const response = await fetch(`${this.aiBaseUrl}/api/chat/send-message`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.INTERNAL_AI_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ 
        userId, 
        message,
        conversationHistory: conversationHistory.slice(-10) // Last 10 messages for context
      })
    });
    
    if (!response.ok) {
      throw new Error(`AI Service error: ${response.status}`);
    }
    
    return response.json() as Promise<{
      content: string;
      metadata: {
        model: string;
        tokens: number;
        responseTime: number;
        contextUsed: boolean;
        conversationLength: number;
      }
    }>;
  }
  
  async getMemoryContext(userId: string) {
    // Retrieve user-specific AI memory and context
  }
  
  async generateContent(type: 'caption' | 'hashtags', data: any) {
    // AI content generation for creators
  }
}

// Usage in apps/api chat controller:
export class ChatController {
  async sendMessage(req: Request, res: Response) {
    const { message } = req.body;
    const userId = req.user.id;
    
    try {
      // 1. Save user message to database
      const userMessage = await saveUserMessage(userId, message);
      
      // 2. Get conversation history
      const history = await getConversationHistory(userMessage.conversationId);
      
      // 3. Get AI response with context
      const aiResponse = await this.aiService.processMessage(userId, message, history);
      
      // 4. Save AI message to database with metadata
      const aiMessage = await saveAIMessage(userMessage.conversationId, aiResponse.content, aiResponse.metadata);
      
      // 5. Return complete conversation update
      res.json({
        success: true,
        userMessage,
        aiMessage,
        conversationId: userMessage.conversationId,
        metadata: aiResponse.metadata
      });
      
    } catch (error) {
      // Advanced fallback handling
      const fallback = await this.fallbackManager.generateContextualFallback(message, userId);
      res.json({ success: true, content: fallback });
    }
  }
}
```

## Advanced Response Format

```typescript
// apps/ai/src/apps/chat/types/chat.ts

export interface AIProcessRequest {
  userId: string;
  message: string;
  conversationHistory?: Array<{
    role: 'user' | 'assistant';
    content: string;
    createdAt: string;
  }>;
  assessmentId?: string;
  assessmentData?: AssessmentContext;
}

export interface AIProcessResponse {
  success: boolean;
  content: string;
  metadata: {
    model: 'gemini-2.0-flash' | 'gemini-pro';
    tokens: number;
    responseTime: number;
    conversationLength: number;
    contextUsed: boolean;
    assessmentIntegrated: boolean;
    fallbackUsed: boolean;
  };
  error?: string;
}
```

## Environment Variables

```bash
# AI App (.env) - Advanced Configuration
GEMINI_API_KEY=...              # Primary AI provider
OPENAI_API_KEY=...              # Backup AI provider
ANTHROPIC_API_KEY=...           # Claude integration

# Database & Memory
DATABASE_URL=...                # Shared Neon PostgreSQL
REDIS_URL=...                   # AI memory and caching

# Internal Security
INTERNAL_AI_TOKEN=...           # Secure APIâ†”AI communication
JWT_SECRET=...                  # Shared auth validation

# Cost Management
AI_COST_LIMIT_DAILY=100         # Daily spending limit
AI_RATE_LIMIT_USER=50           # Messages per user per hour

# Assessment Integration
ASSESSMENT_SERVICE_URL=...      # Assessment service endpoint
ASSESSMENT_API_KEY=...          # Assessment service auth
```

## Vercel Configuration

```json
// apps/ai/vercel.json
{
  "version": 2,
  "builds": [
    {
      "src": "src/server.ts",
      "use": "@vercel/node",
      "config": {
        "maxLambdaSize": "50mb"
      }
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "/src/server.ts"
    }
  ],
  "env": {
    "NODE_ENV": "production"
  },
  "functions": {
    "src/server.ts": {
      "maxDuration": 30
    }
  }
}
```

## Future AI Evolution (2025-2027)

### **Phase 1 (Post-MVP 2025)**
- âœ… Advanced conversation context management
- âœ… Assessment integration for personalized responses
- âœ… Dynamic prompt building system
- âœ… Complex fallback and error handling

### **Phase 2 (2025-2026)**
- ðŸ”„ Advanced memory system with personality evolution
- ðŸ”„ AI content generation for creators
- ðŸ”„ Multi-provider AI routing (Gemini + OpenAI + Claude)

### **Phase 3 (2026-2027)**
- ðŸš€ AI agents for automated posting and engagement
- ðŸš€ Multimodal AI (video, voice, image generation)
- ðŸš€ Custom model training on creator content
- ðŸš€ Real-time AI collaboration tools

### **Phase 4 (2027+)**
- ðŸš€ AI-powered creator monetization optimization
- ðŸš€ Predictive content performance analysis
- ðŸš€ Automated fan engagement and community management
- ðŸš€ AI-generated NFTs and digital collectibles

## Testing Strategy

```typescript
// Advanced AI testing
npm run test "apps/ai"                    # All AI tests
npm run test "chat"                       # Chat functionality
npm run test "memory"                     # Memory system
npm run test "ConversationProcessor"      # Complex conversation logic
npm run test "AssessmentHandler"          # Assessment integration
npm run test "PromptBuilder"              # Dynamic prompt building
npm run test "GeminiService"              # AI provider integration

// Integration testing
npm run test:dev                          # Local SQLite testing
npm run test:prod                         # Production Neon testing
npm run test:ai-integration               # Cross-app APIâ†”AI testing
npm run test:assessment-integration       # Assessment service testing
```

## Cost Optimization

### **Smart Provider Routing**
```typescript
// Route queries to most cost-effective AI model
- Simple questions â†’ Gemini (cheapest)
- Complex reasoning â†’ GPT-4 (most capable)
- Creative content â†’ Claude (best for writing)
- Assessment-related â†’ Specialized prompts
```

### **Advanced Caching Strategy**
```typescript
// Cache expensive AI responses
- FAQ responses cached for 7 days
- User personality profiles cached for 24 hours
- Content suggestions cached for 1 hour
- Assessment-based responses cached per user
```

### **Usage Monitoring**
```typescript
// Track and optimize AI costs
- Per-user usage limits
- Cost alerts and budgeting
- Model performance vs cost analysis
- Assessment integration cost tracking
```

## Security & Privacy

### **Data Protection**
- User memory encrypted at rest
- AI conversations logged but anonymized
- GDPR compliance for AI-generated data
- Assessment data protected and anonymized

### **Internal API Security**
- Bearer token authentication between APIâ†”AI
- Rate limiting per user and endpoint
- Request validation and sanitization
- Assessment service authentication

## Benefits of Advanced AI Architecture

### **ðŸ’° Cost Management**
- Independent scaling based on AI usage
- Dedicated cost monitoring and optimization
- Different compute requirements from main API
- Assessment integration cost optimization

### **ðŸš€ Performance**
- AI processing doesn't slow down core API
- Optimized for AI workloads (longer timeouts, more memory)
- Can use specialized AI infrastructure when needed
- Complex conversation processing isolated

### **ðŸ”® Future-Proof**
- Easy to integrate new AI providers and models
- Advanced AI features won't complicate main API
- Can evolve AI architecture independently
- Assessment integration ready for expansion

### **âš¡ Development**
- AI team can work independently
- Easier to test and debug AI-specific features
- Clear separation of concerns
- Assessment integration properly isolated 